---
title: CreelAnalysis_skeleton
output:
  html_document:
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

# setup 

```{r set_options, echo = FALSE, message = FALSE}
options(width = 100)
knitr::opts_chunk$set(message = FALSE)
set.seed(123)
```

```{r GetHolidays}
#appears based on timeDate...
GetHolidays <- function(x) { 
  years = as.POSIXlt(x)$year+1900 
  years = unique(years) 
  holidays <- NULL 
  for (y in years) { 
    holidays <- c(holidays, as.character(USNewYearsDay(y))) 
    holidays <- c(holidays, as.character(USMLKingsBirthday(y)))
    holidays <- c(holidays, as.character(USPresidentsDay(y)))
    holidays <- c(holidays, as.character(USMemorialDay(y))) 
    holidays <- c(holidays, as.character(USIndependenceDay(y))) 
    holidays <- c(holidays, as.character(USLaborDay(y))) 
    holidays <- c(holidays, as.character(USVeteransDay(y))) 
    holidays <- c(holidays, as.character(USThanksgivingDay(y))) 
    holidays <- c(holidays, as.character(timeDate(as.character(.nth.of.nday(y, 11, 5, 4))))) #Black Friday
    holidays <- c(holidays, as.character(USChristmasDay(y))) 
  } 
  holidays = as.Date(holidays,format="%Y-%m-%d") 
  #ans = x %in% holidays 
  return(holidays) 
} 
```

```{r not_in}
"%!in%" <- function(x,table) match(x,table, nomatch = 0) == 0
#DA: if declaring rather than just '!', why not '%!in%' <- function(x,y) {!('%in%'(x,y))} ?
```

```{r using}
#this function appears unused (ironically enough)?
using <- function(...) {
    libs <- unlist(list(...))
    req <- unlist(lapply(libs, require, character.only=TRUE, quietly=TRUE))
    need <- libs[req==FALSE]
    if(length(need) > 0){ 
        install.packages(need)
        lapply(need, require, character.only=TRUE, quietly=TRUE)
    }
}  
```

```{r install_or_load_pack}
install_or_load_pack <- function(pack){
  create.pkg <- pack[!(pack %in% installed.packages()[, "Package"])]
  if (length(create.pkg))
    install.packages(create.pkg, dependencies = TRUE)
  sapply(pack, require, character.only = TRUE)
}
```

```{r load_pkgs, message = FALSE, warning = FALSE, echo = FALSE,results = "hide"}
# Load packages, install and load if not already
packages_list<-c(
  "tidyverse", "lubridate", 
  "plyr", "reshape2", #really needed?

  "devtools", "here", 

  "timeDate", #check conflicts with lubridate?
  "rstan", "shinystan",
  "loo", 

  "readxl",
  "xlsx", #really needed?
  
  "tinytex",
  "kableExtra",
  "cowplot",
  "ggpubr", 
  "RColorBrewer",

  "chron",
  "suncalc", 
  "data.table", #really needed?
  "MASS"
)
install_or_load_pack(pack = packages_list)
```

# data prep

```{r load_lut, message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
#  source(paste0(wd_source_files, "/Load_LUTs.R"))

wd_LUTs <-"lookup tables"       # Location of look-up tables (maybe could be merged with data files??)

#effort_xwalk_filename<-"02_Crosswalk_Table_for_Index_TieIn_Sections_2021-02-04.csv"
effort_xwalk_filename<-"02_Crosswalk_Table_for_Index_TieIn_Sections_2019-01-10.csv"
effort_xwalk<-read.csv(file.path(wd_LUTs,effort_xwalk_filename)) #Effort Section table

river_loc_filename<-"02_River.Locations_2019-01-07.csv"
river_loc<-read.csv(file.path(wd_LUTs,river_loc_filename)) #River Location table

creel_models_filename<-"02_Creel_Models_2021-01-20.csv"
creel_models<-read.csv(file.path(wd_LUTs,creel_models_filename))#Creel model file names and descriptions

```

```{r data_prep_effort}
#source(paste0(wd_source_files, "/Import_Skagit_Creel_Data_and_Format.R"))
wd_data   <-"data"              # Location where data files are stored

effort_file_name <- "03_Effort_dat - 2019_Skagit_creel_JSH_thru_4-30-19.csv"   #"Effort_dat_2021_steelhead_04132021.csv"
interview_file_name <- "03_Interview-dat_2019-Skagit_JSH_thru_4-30-2019.csv" #"Interview_dat_2021_steelhead_cleaned_04132021.csv"


effort.dat<-read.csv(paste(wd_data, effort_file_name, sep="/"), header=TRUE)
effort.dat<-effort.dat[is.na(effort.dat$StreamName)==FALSE,]
effort.dat$Effort.Index<-0
for(i in 1:nrow(effort.dat)){effort.dat$Effort.Index[i]<-i}

#Format Date and Time data
effort.dat$Survey_Date.Formatted<-as.Date(effort.dat$Survey_Date, format="%m/%d/%Y")
effort.dat$Effort_StartTime.Formatted<-format(strptime(paste(effort.dat$Survey_Date.Formatted, effort.dat$Effort_StartTime, sep=" "), "%Y-%m-%d %I:%M %p"), '%H:%M:%S')
effort.dat$Decimal_Effort_Start_Time<-round(as.numeric(chron::times(effort.dat$Effort_StartTime.Formatted))*24, 3)
effort.dat$Effort_EndTime.Formatted<-format(strptime(paste(effort.dat$Survey_Date.Formatted, effort.dat$Effort_EndTime, sep=" "), "%Y-%m-%d %I:%M %p"), '%H:%M:%S')
effort.dat$Decimal_Effort_End_Time<-round(as.numeric(chron::times(effort.dat$Effort_EndTime.Formatted))*24, 3)

effort.dat %>% select(starts_with("Effort"), starts_with("Decimal")) %>% tibble()

#Calculate total shore and total bank anglers from the tie-in survey data 
effort.dat$Floating_Angler_Count<-as.numeric(effort.dat$Shore_Boat_Motor + effort.dat$Shore_Drift_Raft + effort.dat$Boat_Boat_Motor + effort.dat$Boat_Drift_Raft)
effort.dat$Shore_Angler_Count<-as.numeric(effort.dat$Shore_No_Craft + effort.dat$Shore_Pontoon)

# #Identify holidays
# all.holidays<-GetHolidays(unique(effort.dat$Survey_Date.Formatted))
# #DA: need to rebuild this, just a Date vector of holidays given year(s)

y=2020
c(
  timeDate::USNewYearsDay(y), 
  timeDate::USMLKingsBirthday(y),
  timeDate::USPresidentsDay(y),
  timeDate::USMemorialDay(y), 
  timeDate::USIndependenceDay(y), 
  timeDate::USLaborDay(y),
  timeDate::USVeteransDay(y), 
  timeDate::USThanksgivingDay(y), 
  timeDate::timeDate(as.character(timeDate::.nth.of.nday(y, 11, 5, 4))), #Black Friday
  timeDate::USChristmasDay(y)
) %>% as.character() %>% as.Date(format="%Y-%m-%d")
    

dir_data <- "data"
fn_data_effort <- "03_Effort_dat - 2019_Skagit_creel_JSH_thru_4-30-19.csv"
effort <- readr::read_csv(file.path(dir_data, fn_data_effort)) %>%
  filter(!is.na(StreamName)) %>%
  rowid_to_column(var = "Effort.Index") 

effort %>%
  mutate(
    Date = as.Date(effort.dat$Survey_Date, format="%m/%d/%Y"),
    across(c(Effort_StartTime, Effort_EndTime), as.character),
    across(c(Effort_StartTime, Effort_EndTime), .fns = list(Dec = function(x) {round(as.numeric(chron::times(x))*24, 3)})),
    Boat = Shore_Boat_Motor + Shore_Drift_Raft + Boat_Boat_Motor + Boat_Drift_Raft,
    Bank = Shore_No_Craft + Shore_Pontoon
  ) %>%
  #select(Survey_Date, Date, starts_with("Eff"))

#dropping apparently redundant class coercion of "count columns",
#if expecting malformatted data? could add back something like across(c(count_cols), as.numeric(as.character(.)))



  
#Create new columns for day, month, week, YearGroup, and Season 
effort.dat<-effort.dat %>% mutate(
  Day = weekdays(Survey_Date.Formatted) 
  , DayType = if_else(weekdays(Survey_Date.Formatted)=="Saturday" | weekdays(Survey_Date.Formatted)=="Sunday" | Survey_Date.Formatted%in%all.holidays, "Weekend", "Weekday")
  , Month = format(Survey_Date.Formatted,"%b")
  , Month.no = as.numeric(format(Survey_Date.Formatted, "%m"))
  , Year = as.numeric(format(Survey_Date.Formatted, "%Y"))
  , Weeknum = as.numeric(format(Survey_Date.Formatted, "%V"))
  , j.date =  as.numeric(format(Survey_Date.Formatted, "%j"))
  , YearGroup = if_else(as.numeric(format(Survey_Date.Formatted, "%j")) >= YearBegin, paste(as.numeric(format(Survey_Date.Formatted, "%Y")), as.numeric(format(Survey_Date.Formatted, "%Y"))+1, sep="-"), paste(as.numeric(format(Survey_Date.Formatted, "%Y"))-1, as.numeric(format(Survey_Date.Formatted, "%Y")), sep="-"))
  , Season = if_else(as.numeric(format(Survey_Date.Formatted, "%j")) >= summerBegin & as.numeric(format(Survey_Date.Formatted, "%j"))<=summerEnd, "Summer", "Winter")
)

#Remove unnecessary columns and rename  
effort.dat$Survey_Date<-NULL; 
effort.dat$Survey_Start_Time<-NULL; effort.dat$Survey_End_Time<-NULL;
effort.dat$Effort_StartTime<-NULL; effort.dat$Effort_EndTime<-NULL;
effort.dat$Shore_No_Craft<-NULL;
effort.dat$Shore_Boat_Motor<-NULL;
effort.dat$Shore_Drift_Raft<-NULL;
effort.dat$Shore_Pontoon<-NULL;
effort.dat$Boat_Boat_Motor<-NULL;
effort.dat$Boat_Drift_Raft<-NULL;

#Rename column headers
names(effort.dat)[names(effort.dat) == 'Survey_Date.Formatted'] <- 'Date'
names(effort.dat)[names(effort.dat) == 'Effort_StartTime.Formatted'] <- 'Effort_StartTime'
names(effort.dat)[names(effort.dat) == 'Decimal_Effort_Start_Time'] <- 'Effort_StartTime_Dec'
names(effort.dat)[names(effort.dat) == 'Effort_EndTime.Formatted'] <- 'Effort_EndTime'
names(effort.dat)[names(effort.dat) == 'Decimal_Effort_End_Time'] <- 'Effort_EndTime_Dec'
names(effort.dat)[names(effort.dat) == 'Car_Count'] <- 'Vehicles'
names(effort.dat)[names(effort.dat) == 'Trailer_Count'] <- 'Trailers'
names(effort.dat)[names(effort.dat) == 'Floating_Angler_Count'] <- 'Boat'
names(effort.dat)[names(effort.dat) == 'Shore_Angler_Count'] <- 'Bank'

#Create a "HeaderID" column that denotes each unique survey date
effort.dat<-effort.dat[order(effort.dat$Date, effort.dat$Survey.Type, effort.dat$Effort_StartTime),]
HeaderID<-c()
for(date in 1:length(unique(effort.dat$Date))){
  sub.date<-effort.dat[effort.dat$Date == unique(effort.dat$Date)[date],]
  sub.HeaderID<-rep(date, nrow(sub.date))
  HeaderID<-c(HeaderID, sub.HeaderID)
}
effort.dat$HeaderID<-HeaderID

#Order columns  
effort.dat<-effort.dat[, c( "Effort.Index", "HeaderID", "StreamName",  "Date", "Surveyor", "Year", "YearGroup", "Season", "Month", "Month.no"
                            , "Weeknum", "j.date", "Day", "DayType"
                            , "Survey.Type", "SectionName", "CountNum", "Effort_StartTime", "Effort_StartTime_Dec", "Effort_EndTime", "Effort_EndTime_Dec"
                            , "Vehicles", "Trailers", "Boat", "Bank")]
```


```{r data_prep_group}
group.dat<-read.csv(paste(wd_data, interview_file_name, sep="/"), header=TRUE)
group.dat<-group.dat[is.na(group.dat$StreamName)==FALSE,]
group.dat$Group.Index<-0
for(i in 1:nrow(group.dat)){group.dat$Group.Index[i]<-i}

#Format Date and Time data
#Survey Date      
group.dat$Survey_Date.Formatted<-as.Date(group.dat$Survey_Date, format="%m/%d/%Y")
#Fishing Start Time  
group.dat$Fishing_Start_Time.Formatted<-format(strptime(paste(group.dat$Survey_Date.Formatted, group.dat$Fishing_Start_Time, sep=" "), "%Y-%m-%d %I:%M %p"), '%H:%M:%S')
group.dat$Decimal_Fishing_Start_Time<-round(as.numeric(times(group.dat$Fishing_Start_Time.Formatted))*24, 3)
#Fishing Interview Time    
group.dat$Fishing_Interview_Time.Formatted<-format(strptime(paste(group.dat$Survey_Date.Formatted, group.dat$Interview_Start_Time, sep=" "), "%Y-%m-%d %I:%M %p"), '%H:%M:%S')
group.dat$Decimal_Fishing_Interview_Time<-round(as.numeric(times(group.dat$Fishing_Interview_Time.Formatted))*24, 3)

#Create temporary "GroupNum" column
for(row in 1:nrow(group.dat)){group.dat$temp.GroupNum[row]<-paste(group.dat$Page..[row], group.dat$Row[row], sep = "-")}

#Create x-walk table for "GroupNum" 
group.num_xwalk<-setNames(as.data.frame(matrix(NA, nrow=0, ncol=3)), c("Date", "temp.GroupNum", "GroupNum"))    
for(date in 1:length(unique(group.dat$Survey_Date.Formatted))){
  sub.date<-group.dat[group.dat$Survey_Date.Formatted == unique(group.dat$Survey_Date.Formatted)[date],]
  
  temp.GroupNum<-unique(sub.date$temp.GroupNum)
  new.GroupNum<-seq(1, length(temp.GroupNum),1)
  sub.date<-as.character(rep(unique(group.dat$Survey_Date.Formatted)[date], length(temp.GroupNum)))
  sub.group.num_xwalk<-setNames(as.data.frame(cbind(sub.date, temp.GroupNum, new.GroupNum)), c("Date", "temp.GroupNum", "GroupNum"))
  
  group.num_xwalk<-rbind(group.num_xwalk, sub.group.num_xwalk)
}

#Create "GroupNum" column
group.dat$GroupNum<-NA
for(row in 1:nrow(group.dat)){
  group.dat$GroupNum[row]<-as.character(group.num_xwalk$GroupNum[as.Date(group.num_xwalk$Date) == as.Date(group.dat$Survey_Date.Formatted)[row] & as.character(group.num_xwalk$temp.GroupNum) == as.character(group.dat$temp.GroupNum)[row]])
}

#Format data types    
group.dat$NumAnglers<-as.numeric(as.character(group.dat$NumAnglers))

#Create new columns for day, month, week, YearGroup, and Season 
group.dat<-group.dat %>% mutate(
  Day = weekdays(Survey_Date.Formatted) 
  , DayType = if_else(weekdays(Survey_Date.Formatted)=="Saturday" | weekdays(Survey_Date.Formatted)=="Sunday" | Survey_Date.Formatted%in%all.holidays, "Weekend", "Weekday")
  , Month = format(Survey_Date.Formatted,"%b")
  , Month.no = as.numeric(format(Survey_Date.Formatted, "%m"))
  , Year = as.numeric(format(Survey_Date.Formatted, "%Y"))
  , Weeknum = as.numeric(format(Survey_Date.Formatted, "%V"))
  , j.date =  as.numeric(format(Survey_Date.Formatted, "%j"))
  , YearGroup = if_else(as.numeric(format(Survey_Date.Formatted, "%j")) >= YearBegin, paste(as.numeric(format(Survey_Date.Formatted, "%Y")), as.numeric(format(Survey_Date.Formatted, "%Y"))+1, sep="-"), paste(as.numeric(format(Survey_Date.Formatted, "%Y"))-1, as.numeric(format(Survey_Date.Formatted, "%Y")), sep="-"))
  , Season = if_else(as.numeric(format(Survey_Date.Formatted, "%j")) >= summerBegin & as.numeric(format(Survey_Date.Formatted, "%j"))<=summerEnd, "Summer", "Winter")
)

#Remove unnecessary columns and rename  
group.dat$Surveyor<-NULL; group.dat$Survey_Date<-NULL; group.dat$Page..<-NULL; group.dat$Row<-NULL; 
group.dat$temp.GroupNum<-NULL
group.dat$County1<-NULL; 
group.dat$Comments<-NULL; 
group.dat$Fishing_Start_Time<-NULL; group.dat$Interview_Start_Time<-NULL; group.dat$Fishing_End_Time<-NULL; 
group.dat$Fishing_time_corrected<-NULL; group.dat$Fishing_End_Time_Corrected<-NULL;
group.dat$Guided.<-NULL; 
group.dat$AnglerType<-NULL;
group.dat$TargetSpecies<-NULL;
group.dat$FishingMethod<-NULL;

#Rename column headers
names(group.dat)[names(group.dat) == 'Survey_Date.Formatted'] <- 'Date'
names(group.dat)[names(group.dat) == 'CompleteTrip'] <- 'Trip.Status'
names(group.dat)[names(group.dat) == 'Fishing_Start_Time.Formatted'] <- 'Start.Time'
names(group.dat)[names(group.dat) == 'Decimal_Fishing_Start_Time'] <- 'Start.Time.Dec'
names(group.dat)[names(group.dat) == 'Fishing_Interview_Time.Formatted'] <- 'Interview.Time'
names(group.dat)[names(group.dat) == 'Decimal_Fishing_Interview_Time'] <- 'Interview.Time.Dec'
names(group.dat)[names(group.dat) == 'AnglerCategory'] <- 'Angler.Type'
names(group.dat)[names(group.dat) == 'Num_Cars'] <- 'Vehicles'
names(group.dat)[names(group.dat) == 'Num_Trailers'] <- 'Trailers'
names(group.dat)[names(group.dat) == 'Qty'] <- 'Count'

#Rename "Trip.Status" values
group.dat$Trip.Status<-as.character(group.dat$Trip.Status)  
group.dat$Trip.Status[group.dat$Trip.Status == "Yes"]<-"C"
group.dat$Trip.Status[group.dat$Trip.Status == "No"]<-"I"
group.dat$Trip.Status[group.dat$Trip.Status == "UNK"]<-"I"

#Rename "Angler.Type" values
group.dat$Angler.Type<-as.character(group.dat$Angler.Type) 
group.dat$Angler.Type[group.dat$Angler.Type == "Shore"]<-"S"
group.dat$Angler.Type[group.dat$Angler.Type == "Boat"]<-"B"

#Replace "Count" values entered as "NA" to "0"
group.dat[c("Count")][is.na(group.dat[c("Count")])] <- 0

#Order columns  
group.dat<-group.dat[, c("Group.Index", "StreamName", "SectionName", "Date", "Year", "YearGroup", "Season", "Month", "Month.no"
                         , "Weeknum", "j.date", "Day", "DayType", "GroupNum"
                         ,"Angler.Type"
                         #, "FishFromBoat", "NumPoles"
                         , "NumAnglers", "Vehicles", "Trailers"
                         , "Start.Time", "Start.Time.Dec","Interview.Time","Interview.Time.Dec" #, "End.Time", "End.Time.Dec"
                         , "Percent_time_fished"
                         , "Trip.Status"
                         , "Species", "Origin", "Fate", "Count"
                         #, "Target.Spp"
                         # , "SectionName", "HeaderID"
)
]

# Create a "header.dat"                                                                                    
#Extract pertenate columns from "effort.dat" to create "header.dat" DF
    effort.dat.for.header<-effort.dat[, c( "StreamName", "Date", "Year", "YearGroup", "Season", "Month", "Month.no"
                               , "Weeknum", "j.date", "Day", "DayType"
                               #, "Survey_StartTime", "Survey_StartTime_Dec", "Survey_EndTime", "Survey_EndTime_Dec" 
                               , "Survey.Type", "Surveyor"
                               #, "NoSurveyCode",  "Comments"
                               , "HeaderID"
                              )]

    header.dat<-effort.dat.for.header[!duplicated(effort.dat.for.header$HeaderID),]

```


```{r user_inputs,message=FALSE, warning=FALSE, echo = FALSE}
### User inputs
#Here we will specify user inputs like the file names of the data, the species, and stream name we would like to analyze data for, etc. using the chunk of code with this heading in the RMD file.
#======================================================
# Specify relative working directories for sub-folders
#======================================================
wd_source_files<-"source files" # Location of source file (code working "behind the scenes")
wd_models  <-"models"           # Location of model files 
wd_outputs <-"results"          # Location of saved output (summary figures/tables and model results)

#======================================================
# Denote data of interest (used to filter data below)
#======================================================
# Specify filter type(s) to extract data by (Enter "Y" or "N")
  by.Year<-      "N" # If "Y", will filter by full calendar year(s) (Jan. 1 - Dec. 31)
  by.YearGroup<- "N" # If "Y", will filter by a "Year Group", which go from May 1st Yr1 - April 30 Yr2
  by.Season<-    "N" # If "Y", will filter by "season", which is either Summer (May 1st - Oct 31st) or Winter (Nov. 1 - April 30)
  by.StreamName<-"Y" # If "Y", will filter by stream name
  by.Date<-      "N" # If "Y", will filter by a date range
  
# Specify date ranges for "Year Groups" and "Seasons"
  YearBegin<-  121 # day of year a "YearGroup" begins (FYI - 121 = May 1st in a non-leap year)
  summerBegin<-121 
  summerEnd<-  304 # FYI - 304 = Oct. 31st (in a non-leap year)
  winterBegin<-305 
  winterEnd<-  120 

# Specify filter unit(s)
  # YearGroup.of.Interest<- c("2017-2018") 
  # Season.of.Interest<-    c("Winter") 
  # Year.of.Interest<-      c("2017") 
  StreamName.of.Interest<-c("Skagit")
  # Begin.Date<-            c("2016-05-01") #Format must be "yyyy-mm-dd"
  # End.Date<-              c("2017-03-31") #Format must be "yyyy-mm-dd"
  
#======================================================
# Denote catch group of interest (species_origin_fate) 
#======================================================  
  catch.group.of.interest<-c("SH_W_R") 

#=========================
# Denote fishery closures
#=========================
# NOTE: if the fishery was ever closed during a season, meaning that no legal fishing should have occurred, we can set estimates of catch and effort to zero for those specific dates/sections using a two-step process:
# First, denote the total number of dates when the fishery was closed (the closure can be for the entire fishery -- all sections -- or a portion of the fishery)
  
  total.closed.dates<-30 # Total number of dates that at least one section of the river was closed 
  
# Second, if "total.closed.dates" >0, denote the specific date(s) and section that was/were closed use the following format 
    # the first column is the list of individual dates (by row) the fishery was closed date (format of date should be "yyyy-mm-dd") 
    # the number of additional columns equals the number of sections identified in the "final.effort.section.xwalk" 
    # the value entered below each section column should be either a 1 if the section was open or 0 if the section was closed
    # create a new line for each individual closure by date
                           #    Date   , Section-1, Section-2
    closed.Dates.Sections<-c("2021-02-03",     0,         0 
                            ,"2021-02-04",     0,         0
                            ,"2021-02-05",     0,         0
                            ,"2021-02-10",     0,         0 
                            ,"2021-02-11",     0,         0
                            ,"2021-02-12",     0,         0
                            ,"2021-02-17",     0,         0
                            ,"2021-02-18",     0,         0
                            ,"2021-02-19",     0,         0
                            ,"2021-02-24",     0,         0
                            ,"2021-02-25",     0,         0
                            ,"2021-02-26",     0,         0
                            ,"2021-03-03",     0,         0
                            ,"2021-03-04",     0,         0
                            ,"2021-03-05",     0,         0
                            ,"2021-03-10",     0,         0
                            ,"2021-03-11",     0,         0
                            ,"2021-03-12",     0,         0
                            ,"2021-03-17",     0,         0
                            ,"2021-03-18",     0,         0
                            ,"2021-03-19",     0,         0
                            ,"2021-03-24",     0,         0
                            ,"2021-03-25",     0,         0
                            ,"2021-03-26",     0,         0
                            ,"2021-03-31",     0,         0
                            ,"2021-04-01",     0,         0
                            ,"2021-04-02",     0,         0
                            ,"2021-04-07",     0,         0
                            ,"2021-04-08",     0,         0
                            ,"2021-04-09",     0,         0
                            ) 
```

```{r extract, message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
# Extract data of interest and format 
  ## add code that shows options for filtering data by date/year/season/location
  source(paste0(wd_source_files, "/05_Extract_Data_of_Interest_and_Calculate_Fields_2019-04-08.R"))


#---------------------------------------------------------------------------------------------------------- -
# (5A) EXTRACT DATA OF INTEREST                                                                            ----
#---------------------------------------------------------------------------------------------------------- - 
  # create new DFs for each data group that will represent subsetted data set 
    effort.dat.formatted<-effort.dat
    group.dat.formatted<-group.dat
    # gearfish.dat.formatted<-gearfish.dat
    header.dat.formatted<-header.dat
    
  #Extract by "YearGroup"?
    if(by.YearGroup == "Y"){
        sub.effort.dat<-effort.dat.formatted[effort.dat.formatted$YearGroup %in% YearGroup.of.Interest,]
        sub.group.dat<-group.dat.formatted[group.dat.formatted$YearGroup %in% YearGroup.of.Interest,]
        # sub.gearfish.dat<-gearfish.dat.formatted[gearfish.dat.formatted$YearGroup %in% YearGroup.of.Interest,]
        sub.header.dat<-header.dat.formatted[header.dat.formatted$YearGroup %in% YearGroup.of.Interest,]
    }else{
        sub.effort.dat<-effort.dat.formatted
        sub.group.dat<-group.dat.formatted
        # sub.gearfish.dat<-gearfish.dat.formatted
        sub.header.dat<-header.dat.formatted
    }

  #Extract by "Season"? (Summer vs. Winter)
    if(by.Season == "Y"){
      sub.effort.dat<-sub.effort.dat[sub.effort.dat$Season %in% Season.of.Interest,]
      sub.group.dat<-sub.group.dat[sub.group.dat$Season %in% Season.of.Interest,]
      # sub.gearfish.dat<-sub.gearfish.dat[sub.gearfish.dat$Season %in% Season.of.Interest,]
      sub.header.dat<-sub.header.dat[sub.header.dat$Season %in% Season.of.Interest,]
    }  
    
  #Extract by "Year"?
    if(by.Year == "Y"){
      sub.effort.dat<-sub.effort.dat[sub.effort.dat$Year %in% as.numeric(Year.of.Interest),]
      sub.group.dat<-sub.group.dat[sub.group.dat$Year %in% as.numeric(Year.of.Interest),]
      # sub.gearfish.dat<-sub.gearfish.dat[sub.gearfish.dat$Year %in% as.numeric(Year.of.Interest),]
      sub.header.dat<-sub.header.dat[sub.header.dat$Year %in% as.numeric(Year.of.Interest),]
    }
    
  #Extract by "StreamName"?
    if(by.StreamName == "Y"){
      sub.effort.dat<-sub.effort.dat[sub.effort.dat$StreamName %in% StreamName.of.Interest,]
      sub.group.dat<-sub.group.dat[sub.group.dat$StreamName %in% StreamName.of.Interest,]
      # sub.gearfish.dat<-sub.gearfish.dat[sub.gearfish.dat$StreamName %in% StreamName.of.Interest,]
      sub.header.dat<-sub.header.dat[sub.header.dat$StreamName %in% StreamName.of.Interest,]
    }
  
  #Extract by Date (Range)?
    if(by.Date == "Y"){
      sub.Date.Range<-seq(as.Date(Begin.Date), as.Date(End.Date), 1)
      sub.effort.dat<-sub.effort.dat[sub.effort.dat$Date %in% sub.Date.Range,]
      sub.group.dat<-sub.group.dat[sub.group.dat$Date %in% sub.Date.Range,]
      # sub.gearfish.dat<-sub.gearfish.dat[sub.gearfish.dat$Date %in% sub.Date.Range,]
      sub.header.dat<-sub.header.dat[sub.header.dat$Date %in% sub.Date.Range,]
    }  


#------------------------------------------------------------------------------------------------------------   
  
#---------------------------------------------------------------------------------------------------------- - 
# (5B) CALCULATE FIELDS                                                                                    ####
#---------------------------------------------------------------------------------------------------------- -    
    #Julian to Calendar Date Table
        month.one<-as.data.frame(matrix(as.character(seq.Date(as.Date(paste(as.numeric(format(min(sub.header.dat$Date), "%Y")), format(min(sub.header.dat$Date), "%m"), "01", sep="-"))
                                                              , as.Date(paste(as.numeric(format(max(sub.header.dat$Date), "%Y")), format(max(sub.header.dat$Date)+30, "%m"), "01", sep="-")),
                                                              by="month"))))
        names(month.one)<-c("Date")
        month.one$j.date<-(as.numeric(format(as.Date(month.one$Date), "%j")))
        month.one$month.day<-format(as.Date(month.one$Date), "%b-%d")
        month.one$month<-format(as.Date(month.one$Date), "%b")
        month.one  
    
    #Define "Section.Name" and "Section.Num" based on cross-walk table
      #Effort data  
        sub.effort.dat$final.Section.Name<-NA; sub.effort.dat$Section.Num<-NA; 
        for(row in 1:nrow(sub.effort.dat)){
          sub.effort.dat$final.Section.Name[row]<-as.character(effort_xwalk$Section.Name[as.character(effort_xwalk$Section.Field) == as.character(sub.effort.dat$SectionName[row]) & as.character(effort_xwalk$YearGroup) == as.character(sub.effort.dat$YearGroup[row])])
          sub.effort.dat$Section.Num[row]<-effort_xwalk$Section.Summ[as.character(effort_xwalk$Section.Field) == as.character(sub.effort.dat$SectionName[row]) & as.character(effort_xwalk$YearGroup) == as.character(sub.effort.dat$YearGroup[row])]
        }  

      #Interview data
        unique(sub.group.dat$SectionName); table(sub.group.dat$SectionName)
        #NOTE: 14 groups had their fishing section denoted as "Both".  Since this is not an option in the lookup table and we want to define as either Skagit or Sauk, I arbitrarily defined the section for these groups as being Skagit
        sub.group.dat[sub.group.dat$SectionName == "Both",]
        sub.group.dat$SectionName[sub.group.dat$SectionName == "Both"]<-"Dalles bridge to Marblemount bridge"
        sub.group.dat$final.Section.Name<-NA; sub.group.dat$Section.Num<-NA
        for(row in 1:nrow(sub.group.dat)){
          sub.group.dat$final.Section.Name[row]<-as.character(effort_xwalk$Section.Name[as.character(effort_xwalk$Section.Field) == as.character(sub.group.dat$SectionName[row]) & as.character(effort_xwalk$YearGroup) == as.character(sub.group.dat$YearGroup[row])])
          sub.group.dat$Section.Num[row]<-effort_xwalk$Section.Summ[as.character(effort_xwalk$Section.Field) == as.character(sub.group.dat$SectionName[row]) & as.character(effort_xwalk$YearGroup) == as.character(sub.group.dat$YearGroup[row])]
        }  

    #Create index effort x-walk table (that will help define "outage" dates by section in "master" creel analysis file)
          sub.river.effort.xwalk<-effort_xwalk[effort_xwalk$StreamName %in% unique(sub.effort.dat$StreamName), ]
          final.effort.section.xwalk<-setNames(as.data.frame(matrix(NA, nrow=length(unique(sub.river.effort.xwalk$Section.Name)), ncol=2)), c("Section.Name", "Section.Num"))
          for(section in 1:length(unique(sub.river.effort.xwalk$Section.Name))){
            final.effort.section.xwalk[section, "Section.Name"]<-as.character(unique(sub.river.effort.xwalk$Section.Name[sub.river.effort.xwalk$Section.Name == unique(sub.river.effort.xwalk$Section.Name)[section]]))
            final.effort.section.xwalk[section, "Section.Num"] <-unique(sub.river.effort.xwalk$Section.Summ[sub.river.effort.xwalk$Section.Name == unique(sub.river.effort.xwalk$Section.Name)[section]])
          }
          final.effort.section.xwalk
              
    # #Calculate time to complete effort count by section
    #     sub.effort.dat$Count.Time.Minutes<-NA
    #     for(row in 1:nrow(sub.effort.dat)){as.numeric(ifelse(is.na(sub.effort.dat$Effort_EndTime_Dec[row])==FALSE & is.na(sub.effort.dat$Effort_StartTime_Dec[row])==FALSE ,sub.effort.dat$Count.Time.Minutes[row]<-round((sub.effort.dat$Effort_EndTime_Dec[row] - sub.effort.dat$Effort_StartTime_Dec[row])*60, 0), NA))}  
      
    #Identify dates when both index and tie-in surveys were completed on same day 
        index.dates<-unique(sub.effort.dat$Date[sub.effort.dat$Survey.Type == "Creel" ])
        tie.dates<-unique(sub.effort.dat$Date[sub.effort.dat$Survey.Type == "TieIn" ])
        paired.dates<-index.dates[index.dates %in% tie.dates]
        
    #Create summary of creel counts to identify which one pairs best with tie-in count
        paired.surveys<-setNames(as.data.frame(matrix(NA, nrow=0, ncol=8)), c("Date", "Count", "Index.Start", "Index.End", "TI.Start", "TI.End", "Start.Diff", "End.Diff"))
        for(date in 1:length(paired.dates)){
          sub.date<-sub.effort.dat[sub.effort.dat$Date == unique(paired.dates)[date],]
          counts<-unique(sub.date$CountNum)
          
          sub.paired.surveys<-setNames(as.data.frame(matrix(NA, nrow=max(sub.effort.dat$CountNum), ncol=8)), c("Date", "Count", "Index.Start", "Index.End", "TI.Start", "TI.End", "Start.Diff", "End.Diff"))
          
          tie.in.data<-sub.date[sub.date$Survey.Type == "TieIn", ]
          
            for(count in 1:length(counts)){
                sub.count<-sub.date[sub.date$CountNum == unique(sub.date$CountNum)[count] & sub.date$Survey.Type == "Creel",]
                
                sub.paired.surveys[count, "Date"]<-as.character(unique(paired.dates)[date])    
                sub.paired.surveys[count, "Count"]<-as.character(unique(sub.date$CountNum)[count])
                sub.paired.surveys[count, "Index.Start"]<-min(sub.count$Effort_StartTime_Dec, na.rm=TRUE)
                sub.paired.surveys[count, "Index.End"]<-max(sub.count$Effort_EndTime_Dec, na.rm=TRUE)
                sub.paired.surveys[count, "TI.Start"]<-min(tie.in.data$Effort_StartTime_Dec, na.rm=TRUE)
                sub.paired.surveys[count, "TI.End"]<-max(tie.in.data$Effort_EndTime_Dec, na.rm=TRUE)
                sub.paired.surveys[count, "Start.Diff"]<-abs(sub.paired.surveys$Index.Start[count] - sub.paired.surveys$TI.Start[count])
                sub.paired.surveys[count, "End.Diff"]<-abs(sub.paired.surveys$Index.End[count] - sub.paired.surveys$TI.End[count] )
            }
          paired.surveys<-rbind(paired.surveys, sub.paired.surveys)
        }  
        paired.surveys

    #Update "CountNum" for TieIn surveys based on summary in "paired.surveys" DF    
        sub.effort.dat$CountNum_New<-NA
        for(row in 1:nrow(sub.effort.dat)){
            ifelse(sub.effort.dat$Survey.Type[row] != "Creel" & as.character(sub.effort.dat$Date[row]) %in% as.character(paired.surveys$Date)
              , sub.effort.dat$CountNum_New[row]<-as.numeric(paired.surveys$Count[paired.surveys$Date == sub.effort.dat$Date[row] & paired.surveys$Start.Diff ==  min(paired.surveys$Start.Diff[paired.surveys$Date == sub.effort.dat$Date[row]])])
              , sub.effort.dat$CountNum_New[row]<-sub.effort.dat$CountNum[row])
        }
        unique(sub.effort.dat$Date[sub.effort.dat$CountNum != sub.effort.dat$CountNum_New])
        
    #Calculate "corrected" End time (if using "Percent_time_fished" field) - unique to Skagit River fishery
        if(any(colnames(sub.group.dat)=="Percent_time_fished")==TRUE){
          sub.group.dat$End.Time.Dec<-NA
          for(row in 1:nrow(sub.group.dat)){
            if(sub.group.dat$Percent_time_fished[row] == 1.0){
              sub.group.dat$End.Time.Dec[row]<-sub.group.dat$Interview.Time.Dec[row]
              
            }else{
              sub.group.dat$End.Time.Dec[row]<-sub.group.dat$Start.Time.Dec[row] + (sub.group.dat$Interview.Time.Dec[row] - sub.group.dat$Start.Time.Dec[row])*sub.group.dat$Percent_time_fished[row] 
            }
          }
        }

    #Calculate "Hours" fished by Group
        #unique(sub.group.dat$Trip.Status)
        if(any(colnames(sub.group.dat)=="Percent_time_fished")==TRUE){
          for(row in 1:nrow(sub.group.dat)){
          sub.group.dat$Hours[row]<-sub.group.dat$End.Time.Dec[row] - sub.group.dat$Start.Time.Dec[row]
          }
        }else{  
        
        for(row in 1:nrow(sub.group.dat)){
          ifelse(sub.group.dat$Trip.Status[row] == "C", sub.group.dat$Hours[row]<-(sub.group.dat$End.Time.Dec[row] - sub.group.dat$Start.Time.Dec[row]) , sub.group.dat$Hours[row]<-(sub.group.dat$Interview.Time.Dec[row] - sub.group.dat$Start.Time.Dec[row]) )
        }
        }
        
          
    #Calculate "Total_Hours" ("Hours" * "NumAnglers") by Group
        #unique(sub.group.dat$Trip.Status)
        for(row in 1:nrow(sub.group.dat)){
          sub.group.dat$Total_Hours[row]<-(sub.group.dat$Hours[row] * sub.group.dat$NumAnglers[row]) 
        }    

    #Combine "Species", "Origin", and "Fate" to create a unique "Catch.Group" index
      for(row in 1:nrow(sub.group.dat)){
        sub.group.dat$Catch.Group[row]<-paste(sub.group.dat$Species[row], sub.group.dat$Origin[row], sub.group.dat$Fate[row], sep = "_")
      }
      # for(row in 1:nrow(sub.gearfish.dat)){
      #   sub.gearfish.dat$Catch.Group[row]<-paste(sub.gearfish.dat$Species[row], sub.gearfish.dat$Origin[row], sub.gearfish.dat$Fate[row], sep = "_")
      # }

```

```{r ready_for_stan, message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
#Run source summary file 
  ## add code that shows options for "catch groups"
  source(paste0(wd_source_files, "/06_Summarize_Effort_and_Catch_Data_for_TimeSeries_Model_2019-04-23.R"))   

##KB note: I will work on updating the code in the "05" and "06" file at some point soon; also, creating a "Import and format" file for data that is from our creel database

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# This file summarizes creel effort and catch data that will then be fed directly into time-series model
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Denote date range of dataset
  Date_Begin<-min(sub.effort.dat$Date) #c("2018-04-01") #Define date as the first day for which you want to make an estimate for the fishery (not necessarily first survey date)
  Date_End<-  max(sub.effort.dat$Date) #c("2018-04-30")    

#---------------------------------------------------------------------------------------------------------- - 
# (6A) Create "all.Dates" x-walk table and create vector for open/closed fishery dates                   ####
#---------------------------------------------------------------------------------------------------------- - 
  #Create cross-walk table "all.Dates" that defines the relative "day" and "week" number based on "Date_Begin" and "Date_End"
      all.Dates<-setNames(as.data.frame(cbind(seq(1, as.numeric(as.Date(Date_End)-as.Date(Date_Begin)+1), 1) , as.character(seq(as.Date(Date_Begin), as.Date(Date_End), 1)))), c("Day", "Date"))
      all.Dates$Year<-as.numeric(format(as.Date(all.Dates$Date), "%Y"))
      all.Dates$Month<-format(as.Date(all.Dates$Date), "%b")
      all.Dates$Month.no<-as.numeric(format(as.Date(all.Dates$Date), "%m"))
      all.Dates$DayofWeek<-weekdays(as.Date(all.Dates$Date)); 
      all.Dates$DayType<-ifelse(all.Dates$DayofWeek=="Saturday"|all.Dates$DayofWeek=="Sunday",1,0)
      all.Dates$DOY<-(as.numeric(format(as.Date(all.Dates$Date), "%j")))
      all.Dates$WeekNum_abs<-as.numeric(format(as.Date(all.Dates$Date), "%V"))
      all.Dates$Yr_Week<-paste(as.numeric(all.Dates$Year), as.numeric(all.Dates$WeekNum_abs), sep="_")
      head(all.Dates)
      
  ##Day Length
      #NOTE: I have specified day length using dawn & dusk, but this can be changed to several other metrics (e.g., sunrise & sunset)
      day.light<-getSunlightTimes(keep=c("dawn", "dusk"), date=seq(min(as.Date(all.Dates$Date)), max(as.Date(all.Dates$Date)),1), lat = river_loc$Lat[river_loc$River == StreamName.of.Interest], lon=river_loc$Long[river_loc$River == StreamName.of.Interest], tz = "America/Los_Angeles")
      day.light$DayL<-as.numeric(day.light[[5]]-day.light[[4]])    

  #Calculate relative day (DayNum_Rel) and week (WeekNum_Rel) columns for "Dates.of.Interest" DF
      #weeknums.abs<-unique(as.numeric(format(as.Date(Dates.of.Interest$Date), "%V")))
      weeknums.abs<-unique(all.Dates$Yr_Week)
      weeknums.rel<-seq(1, length(weeknums.abs),1)
      weeknum_xwalk<-setNames(as.data.frame(cbind(weeknums.abs, weeknums.rel)), c("Yr_Week", "weeknum"))
      for(row in 1:nrow(all.Dates)){all.Dates$Weeknum_Rel[row]<-as.numeric(weeknum_xwalk$weeknum[as.character(weeknum_xwalk$Yr_Week) == as.character(all.Dates$Yr_Week[row])])}
  
      all.Dates$Daynum_Rel<-all.Dates$Day
      # daynums.abs<-unique(all.Dates$Date)
      # daynum.rel<-seq(1, length(daynums.abs),1)
      # daynum_xwalk<-setNames(as.data.frame(cbind(daynums.abs, daynum.rel)), c("DOY", "daynum"))
      # for(row in 1:nrow(all.Dates)){all.Dates$Daynum_Rel[row]<-as.character(daynum_xwalk$daynum[as.character(daynum_xwalk$DOY) == as.character(all.Dates$Day)[row]])}
      head(all.Dates)
        
  #Using user defined "closed.Dates", create vector of data denoting whether the fishery was open (1) or closed (1E-9) by section 
          if(total.closed.dates==0){
              fishery.open.closed<-setNames(cbind(all.Dates$Date, as.data.frame(matrix(1, nrow=nrow(all.Dates), ncol=nrow(final.effort.section.xwalk)))), c("Date", as.character(final.effort.section.xwalk$Section.Name)))
          }else{
              fishery.open.closed<-setNames(cbind(all.Dates$Date, as.data.frame(matrix(1, nrow=nrow(all.Dates), ncol=nrow(final.effort.section.xwalk)))), c("Date", as.character(final.effort.section.xwalk$Section.Name)))
              closed.Dates.DF<-setNames(as.data.frame(matrix(closed.Dates.Sections, nrow=total.closed.dates, ncol=nrow(final.effort.section.xwalk)+1, byrow=TRUE)), c("Date", final.effort.section.xwalk$Section.Name))
              
              for(date in 1:nrow(all.Dates)){
                  if(all.Dates$Date[date] %in% closed.Dates.DF$Date){
                    for(section in 1:nrow(final.effort.section.xwalk)){
                        
                      fishery.open.closed[date, final.effort.section.xwalk$Section.Name[section]]<-as.numeric(as.character(closed.Dates.DF[as.Date(closed.Dates.DF$Date) == as.Date(all.Dates$Date)[date], final.effort.section.xwalk$Section.Name[section]]))
                    }
                  }
              }
          }
      fishery.open.closed
      fishery.open.closed[fishery.open.closed == 0] <- 1E-6
      fishery.open.closed$Date<-NULL

#---------------------------------------------------------------------------------------------------------- - 
# (6B) SUMMARIZE EFFORT DATA                                                                     ####
#---------------------------------------------------------------------------------------------------------- -         
  # Sub-set effort data based on defined "Date_Begin" and "Date_End"
    final_effort.dat<-sub.effort.dat[as.Date(sub.effort.dat$Date) %in% as.Date(all.Dates$Date),]
    length(unique(final_effort.dat$Date))

  # Sub-set Day Length data frame based on defined Date_Begin" and "Date_End"
    head(day.light)
    nrow(day.light) 
    DayL<-day.light[as.Date(day.light$date) %in% as.Date(all.Dates$Date),] #>= as.Date(Date_Begin) & as.Date(day.light$date) <= as.Date(Date_End), c("date", "DayL")]
    nrow(DayL)

  #Define aggregates of effort data
    creel.dates<-unique(final_effort.dat$Date)                    # Unique creel survey dates
    n_creel.dates<-length(creel.dates)                            # Number of unique creel survey dates
    
    survey.types<-unique(final_effort.dat$Survey.Type)            # Unique creel survey types (should be two - "Creel" & "TieIn")
    n_survey.types<-length(survey.types)                          # Number of unique creel survey types 
    
    counts<-as.vector(unique(final_effort.dat$CountNum[order(as.numeric(final_effort.dat$CountNum))])) # Unique counts (should be two - "1" & "2")
    n_counts<-length(counts)                                      # Number of unique (index) counts 
    
    gear.types<-c("Vehicles", "Trailers", "Bank", "Boat")         # Unique gear types (of interest)
    gear.short<-c("V", "T", "S", "B")                                       # Abbreviations for gear groups
    n_gear.types<-length(gear.types)                              # Number of unique gear types
    
    sections<-as.vector(unique(final_effort.dat$Section.Num[order(as.numeric(final_effort.dat$Section.Num))])) # Unique survey sections
    section.xwalk<-setNames(as.data.frame(matrix(c(unique(final_effort.dat$Section.Num), unique(final_effort.dat$final.Section.Name)),nrow=length(sections), ncol=2, byrow=FALSE)), c("Section_Num", "Section_Name"))
    n_sections<-length(sections)

  #Create summary effort data frame ("effort_samp_sum") to fill  
    effort_samp_sum<-setNames(as.data.frame(matrix(NA, nrow=0, ncol=8)), c("Date", "Survey", "Gear_Alpha", "Section", "CountNum", "CountTime", "CountTime_Dec", "Count"))
    
  #Loop through effort data (of interest) and fill  
    for(date in 1:n_creel.dates){
        sub_date<-final_effort.dat[final_effort.dat$Date == creel.dates[date], ]
        
        for(survey_type in 1:n_survey.types){
          sub_date.survey<-sub_date[sub_date$Survey.Type == survey.types[survey_type], ]
          
            for(section in 1:n_sections){
              sub_date.survey.section<-sub_date.survey[sub_date.survey$Section.Num == sections[section],]
              
              for(count in 1:n_counts){
                sub_date.survey.section.count<-sub_date.survey.section[sub_date.survey.section$CountNum_New == counts[count], ]
            
                sub.effort_samp_sum<-as.data.frame(matrix(NA, nrow=n_gear.types, ncol=8))
                names(sub.effort_samp_sum)<-c("Date", "Survey", "Gear_Alpha", "Section", "CountNum", "CountTime", "CountTime_Dec", "Count")
                
                for(gear in 1:n_gear.types){
                    sub.effort_samp_sum[gear, "Date"]<-as.character(creel.dates[date])
                    sub.effort_samp_sum[gear, "Survey"]<-as.character(survey.types[survey_type])
                    sub.effort_samp_sum[gear, "Gear_Alpha"]<-as.character(gear.types[gear])
                    sub.effort_samp_sum[gear, "Section"]<- as.character(sections[section])
                    sub.effort_samp_sum[gear, "CountNum"]<-counts[count]
                    sub.effort_samp_sum[gear, "CountTime"]<-ifelse(nrow(sub_date.survey.section.count)==0, NA, sub_date.survey.section.count$Effort_StartTime)
                    sub.effort_samp_sum[gear, "CountTime_Dec"]<-ifelse(nrow(sub_date.survey.section.count)==0, NA, sub_date.survey.section.count$Effort_StartTime_Dec)
                    sub.effort_samp_sum[gear, "Count"]<-ifelse(nrow(sub_date.survey.section.count)==0, NA, sum(sub_date.survey.section.count[, gear.types[gear]] ) )
                    
                }
                effort_samp_sum<-rbind(effort_samp_sum, sub.effort_samp_sum)
              }
            }
        }
    }
    effort_samp_sum[1:20,]

  #Add columns to "effort_samp_sum" that make variable values numeric 
    ##Day   
      effort_samp_sum$Day<-NA; for(row in 1:nrow(effort_samp_sum)){ effort_samp_sum$Day[row]<-as.numeric(as.character(all.Dates$Daynum_Rel[all.Dates$Date == effort_samp_sum$Date[row]]))}
        
    ##DayType    
      effort_samp_sum$DayType<-NA; for(row in 1:nrow(effort_samp_sum)){ effort_samp_sum$DayType[row]<-as.numeric(as.character(all.Dates$DayType[all.Dates$Date == effort_samp_sum$Date[row]]))}
        
    ##Weeknum
      effort_samp_sum$Week<-NA; for(row in 1:nrow(effort_samp_sum)){ effort_samp_sum$Week[row]<-as.numeric(as.character(all.Dates$Weeknum_Rel[all.Dates$Date == effort_samp_sum$Date[row]]))}
      n_week<-length(unique(effort_samp_sum$Week))  
        
    ##Gear
      gear.types; gear.short
      all.Gear<-setNames(as.data.frame(cbind(seq(1, length(gear.types),1), gear.types, gear.short)), c("Gear", "Gear_Alpha", "Gear_Short"))
      effort_samp_sum$Gear<-NA; for(row in 1:nrow(effort_samp_sum)){ effort_samp_sum$Gear[row]<-as.numeric(as.character(all.Gear$Gear[all.Gear$Gear_Alpha == effort_samp_sum$Gear_Alpha[row]]))}

    ##UniqueID (based on "Week"_"Day"_"CountNum")  
      effort_samp_sum$UniqueID<-NA; for(row in 1:nrow(effort_samp_sum)){effort_samp_sum$UniqueID[row]<-paste(effort_samp_sum$Week[row], effort_samp_sum$Day[row], effort_samp_sum$CountNum[row], sep = "_")}
      head(effort_samp_sum)
      
    ##CountNum by Week
        CountNum_byWeek_Xwalk<-setNames(as.data.frame(matrix(NA,nrow=0, ncol=2)), c("UniqueID", "CountNum_Week"))
        for(week in 1:length(unique(effort_samp_sum$Week))){
          sub.week<-effort_samp_sum[effort_samp_sum$Week == unique(effort_samp_sum$Week)[week],]
          
          sub.CountNum_byWeek_Xwalk<-setNames(as.data.frame(matrix(NA,nrow=length(unique(sub.week$UniqueID)), ncol=2)), c("UniqueID", "CountNum_Week"))
          
          for(ID in 1:length(unique(sub.week$UniqueID))){
            sub.CountNum_byWeek_Xwalk[ID, "UniqueID"]<-as.character(unique(sub.week$UniqueID)[ID])
            sub.CountNum_byWeek_Xwalk[ID, "CountNum_Week"]<-ID
                      }
          CountNum_byWeek_Xwalk<-rbind(CountNum_byWeek_Xwalk, sub.CountNum_byWeek_Xwalk)
        }
        head(CountNum_byWeek_Xwalk)
        
    effort_samp_sum$CountNum_Week<-NA; for(row in 1:nrow(effort_samp_sum)){effort_samp_sum$CountNum_Week[row]<- CountNum_byWeek_Xwalk$CountNum_Week[CountNum_byWeek_Xwalk$UniqueID == effort_samp_sum$UniqueID[row]]}
    
    # #Summarize data to make sure "CountNum_Week" summary worked
    #   effort_samp_sum %>%group_by(Week) %>%summarise(n_distinct(Date)) %>% print(n=50)
    #   effort_samp_sum %>%group_by(Week) %>%summarise(n_distinct(CountNum_Week))
    #   effort_samp_sum %>%group_by(Week) %>% distinct(Date)%>% print(n=60)

  #Index samples
    I_samp<-effort_samp_sum[effort_samp_sum$Survey == "Creel", c("Day", "DayType", "Week", "Gear", "Section", "CountNum", "CountNum_Week", "Count")]
    I_samp<-I_samp[!is.na(I_samp$Count),] #Drop rows where Count == "NA"
    I_samp$Section<-as.numeric(I_samp$Section)
    V_samp<-I_samp[I_samp$Gear==1,] # & !is.na(I_samp$Count)
    T_samp<-I_samp[I_samp$Gear==2,]
    A_samp<-I_samp[I_samp$Gear==3 | I_samp$Gear==4,]

  #Tie-In samples
    E_s_samp<-effort_samp_sum[effort_samp_sum$Survey == "TieIn", c("Day", "DayType", "Week", "Gear", "Section", "CountNum", "CountNum_Week", "Count")]
    E_s_samp<-E_s_samp[is.na(E_s_samp$Count)==FALSE,] #Drop rows where Count == "NA"
    E_s_samp$Section<-as.numeric(E_s_samp$Section)
    
  #Re-index gear-types for tie-in counts (bank(3) == 1; boat(4) ==2)  
    E_s_samp$Gear[E_s_samp$Gear == 3] <- 1; 
    E_s_samp$Gear[E_s_samp$Gear == 4] <- 2;
    
#---------------------------------------------------------------------------------------------------------- - 
# (6C) SUMMARIZE cATCH DATA                                                                     ####
#---------------------------------------------------------------------------------------------------------- - 
    head(sub.group.dat)
    #head(sub.gearfish.dat)
  
  # Sub-set group (interview) data based on defined "Date_Begin" and "Date_End"
    final_group.dat<-sub.group.dat[as.Date(sub.group.dat$Date) %in% as.Date(all.Dates$Date),]
    length(unique(final_group.dat$Date))    
  
  #Order data by Date and GroupNum
    final_group.dat<-final_group.dat[order(as.Date(final_group.dat$Date), final_group.dat$GroupNum),]
    #sub.gearfish.dat<-sub.gearfish.dat[order(as.Date(sub.gearfish.dat$Date), sub.gearfish.dat$GroupNum),]
    
  #Drop "final_group.dat" data if "Start.Time" is <NA>, "Interview.Time" is <NA> for groups with Trip.Status == I, or "End.Time" is <NA> for groups with Trip.Status == C
    drop.group.indices<-c(final_group.dat$GroupNum[is.na(final_group.dat$Start.Time)==TRUE] #Missing Start Time
      , final_group.dat$GroupNum[is.na(final_group.dat$Interview.Time)==TRUE & final_group.dat$Trip.Status=="I" ] #Missing Interview Time for Incomplete Trips
      , final_group.dat$GroupNum[is.na(final_group.dat$Start.Time)==FALSE & is.na(final_group.dat$End.Time.Dec)==TRUE & final_group.dat$Trip.Status=="C" ] #Missing Interview Time for Completed Trips
      )
    length(drop.group.indices)
    ifelse(length(drop.group.indices)==0, final_group.dat<-final_group.dat, final_group.dat<-final_group.dat[as.character(final_group.dat$GroupNum) %!in% as.character(drop.group.indices),])

  #Drop rows if hours fished by a Group is Negative 
    nrow(final_group.dat[final_group.dat$Hours<=0 | is.na(final_group.dat$Hours)==TRUE,])
    final_group.dat<-final_group.dat[final_group.dat$Hours>0 & is.na(final_group.dat$Hours)==FALSE,]
    
  #Assign values for "Count" - prior to eDate capture, "Count" was not a entered field and therefore all rows = NA
    final_group.dat$Count_New<-final_group.dat$Count
    
  #Summarize group level data by "Date" and "GroupNum" and "final.Section.Name" (NOTE: needed to add section for Skagit data because anglers could report fishing in multiple sections)
    for(row in 1:nrow(final_group.dat)){final_group.dat$temp.Group[row]<-paste(final_group.dat$Date[row], final_group.dat$GroupNum[row], final_group.dat$final.Section.Name[row], sep="_")}
    #uniq.Groups<-unique(final_group.dat$temp.Group)
    sub.unique.group.dat<-final_group.dat[!duplicated(final_group.dat$temp.Group),]
    
  #Identify catch groupings of interest
    unique(final_group.dat$Catch.Group)
    n_catch.groups<-length(catch.group.of.interest)
    
  #Summarize Catch of Interest by Angler Group
    catch.dat_byGroup<-setNames(as.data.frame(matrix(NA, nrow=length(unique(final_group.dat$temp.Group))*length(catch.group.of.interest), ncol=3)), c("temp.Group", "Catch.Group", "Count"))
    for(group in 1:length(unique(final_group.dat$temp.Group))){
        sub.Group.Index<-final_group.dat[final_group.dat$temp.Group == unique(final_group.dat$temp.Group)[group], ]
      
        for(spp.group in 1:length(catch.group.of.interest)){
          
          catch.dat_byGroup[(group-1) * n_catch.groups  + spp.group, "temp.Group"]<-unique(final_group.dat$temp.Group)[group]
          catch.dat_byGroup[(group-1) * n_catch.groups  + spp.group, "Catch.Group"]<-catch.group.of.interest[spp.group]
          catch.dat_byGroup[(group-1) * n_catch.groups  + spp.group, "Count"]<-sum(sub.Group.Index$Count_New[sub.Group.Index$Catch.Group==catch.group.of.interest[spp.group]])
        }
    }
    head(catch.dat_byGroup)
    tail(catch.dat_byGroup)
    aggregate(catch.dat_byGroup$Count ~ catch.dat_byGroup$Catch.Group, FUN=sum)
    
  #Summarize "catch.dat_byUniqueGroup"
   catch.dat_byUniqueGroup<- catch.dat_byGroup %>% spread(Catch.Group, Count) 
   head(catch.dat_byUniqueGroup)
   length(unique(catch.dat_byUniqueGroup$temp.Group)); nrow(catch.dat_byUniqueGroup)
   
  #Create new datasets by joining "sub.unique.group.dat" and "catch.dat_byUniqueGroup" using "temp.Group"
    group.fish.dat<-left_join(sub.unique.group.dat, catch.dat_byUniqueGroup, by="temp.Group")
    length(unique(group.fish.dat$Group.Index)); nrow(group.fish.dat)
    head(group.fish.dat) 
    
  # #Assign 0s (catch) for rows with values == NA (NOTE: this happens if a group was interviewed but no gear/fish entry was created either in database and/or on back of datasheet) 
  #   group.fish.dat[, catch.group.of.interest][is.na(group.fish.dat[, catch.group.of.interest])]<-0

  #Sub-set data of interest
    Group_Catch<-group.fish.dat[, c("Date", "Weeknum", "DayType", "Angler.Type", "temp.Group",  "Section.Num", "Hours", "Trip.Status", "NumAnglers", "Vehicles", "Trailers", "Total_Hours", catch.group.of.interest)]
    head(Group_Catch)

  #Drop group interviews if "Hours" less than threshold AND "Trip.Status" == I
    drop.short.trips<-"Y" #Enter "Y" (yes) or "N" (no)
    min.fish.threshold<-0.5 #If "Y", enter as minimum trip length threshold for groups that are still fishing (i.e., Trip.Status == "I")
    if(drop.short.trips == "Y"){
        Group_Catch<-Group_Catch[Group_Catch$Trip.Status == "C" |  (Group_Catch$Trip.Status == "I" & Group_Catch$Hours >= min.fish.threshold),]
    }
    
  #Drop data if "Angler.Type" is equal to "NA"
    Group_Catch<-Group_Catch[is.na(Group_Catch$Angler.Type)==FALSE,]
    
  #Sub-set catch data based on defined Date_Begin" and "Date_End"
    final_Group_Catch.dat<-Group_Catch[as.Date(Group_Catch$Date) >= as.Date(Date_Begin) & as.Date(Group_Catch$Date) <= as.Date(Date_End), ]
    length(unique(final_Group_Catch.dat$Date)) 
    length(unique(final_effort.dat$Date))
    
  #Add columns to "final_Group_Catch.dat" that make variable values numeric 
    ##Day
        final_Group_Catch.dat$Day<-NA; for(row in 1:nrow(final_Group_Catch.dat)){ final_Group_Catch.dat$Day[row]<-as.numeric(as.character(all.Dates$Daynum_Rel[as.Date(all.Dates$Date) == as.Date(final_Group_Catch.dat$Date[row])]))}
    ##DayType    
        final_Group_Catch.dat$DayType<-NA; for(row in 1:nrow(final_Group_Catch.dat)){ final_Group_Catch.dat$DayType[row]<-as.numeric(as.character(all.Dates$DayType[as.Date(all.Dates$Date) == as.Date(final_Group_Catch.dat$Date[row])]))}
    ##Gear
        final_Group_Catch.dat$Gear<-NA; for(row in 1:nrow(final_Group_Catch.dat)){ final_Group_Catch.dat$Gear[row]<-as.numeric(as.character(all.Gear$Gear[all.Gear$Gear_Short == final_Group_Catch.dat$Angler.Type[row]]))}
    ##Group
      all.Groups<-setNames(as.data.frame(cbind(seq(1, nrow(final_Group_Catch.dat), 1) , final_Group_Catch.dat$temp.Group)), c("Group", "temp.Group"))
      final_Group_Catch.dat$Angler<-NA; for(row in 1:nrow(final_Group_Catch.dat)){ final_Group_Catch.dat$Angler[row]<-as.numeric(as.character(all.Groups$Group[as.character(all.Groups$temp.Group) == as.character(final_Group_Catch.dat$temp.Group[row])]))}
          #NOTE: if you get a warning here there may be two groups with the same "ID" which would result from a data entry error
      
    ##Weeknum
      final_Group_Catch.dat$Week<-NA; for(row in 1:nrow(final_Group_Catch.dat)){ final_Group_Catch.dat$Week[row]<-as.numeric(as.character(all.Dates$Weeknum_Rel[as.Date(all.Dates$Date) == as.Date(final_Group_Catch.dat$Date[row])]))}

  #Catch samples (to estimate CPUE)
    c_samp<-final_Group_Catch.dat[is.na(final_Group_Catch.dat$Hours)==FALSE, c("Day", "DayType", "Week", "Gear", "Angler", "Section.Num", "Total_Hours", catch.group.of.interest)]
    head(c_samp)
    nrow(c_samp)
    
  #Angler samples (to estimate boats & trailers per angler)
    a_samp<-final_Group_Catch.dat[is.na(final_Group_Catch.dat$NumAnglers)==FALSE & is.na(final_Group_Catch.dat$Vehicles)==FALSE & is.na(final_Group_Catch.dat$Trailers)==FALSE, c("Day", "DayType", "Week", "Gear", "Angler", "Section.Num", "NumAnglers", "Vehicles", "Trailers")]
    head(a_samp)
    nrow(a_samp)  
    
  #Re-index gear-types for interview anglers (bank(3) == 1; boat(4) ==2)  
    c_samp$Gear[c_samp$Gear == 3] <- 1; c_samp$Gear[c_samp$Gear == 4] <- 2;
    a_samp$Gear[a_samp$Gear == 3] <- 1; a_samp$Gear[a_samp$Gear == 4] <- 2;
    gear.xwalk<-setNames(as.data.frame(matrix(c(c(1,2), as.character(all.Gear$Gear_Alpha[c(3:4)])),nrow=2, ncol=2, byrow=FALSE)), c("Gear_Num", "Gear_Name"))

  #total hours creel and total catch sample a per day/gear/section to compare with effort
    C_sample<-c_samp%>%
      group_by(Day,Gear,Section.Num)%>%
      summarise(Total_Hours=sum(Total_Hours),Total_Catch=sum((!!as.name(catch.group.of.interest))))%>%
      filter(Total_Hours>0)
    #sapply(as.data.frame(lapply(JS_stats_ALL, "[[", "ni")), function(x) as.numeric(as.character(x)))
#=====================================================================================================================
# (6E) make stan data
#=====================================================================================================================
standat<-list(
  D=nrow(all.Dates),                                               # total number of days in the fishery we are estimating catch for
  G=length(unique(I_samp$Gear)),                                   # total number of unique gear types (#previously G=n_gear.types)
  S=n_sections,                                                    # total number of census/tie in Sections
  H=max(as.numeric(V_samp$CountNum), as.numeric(T_samp$CountNum)), # max. number of index effort counts completed on a single survey day

  #day attributes
  w=all.Dates$DayType,                                             # an indexing column with length equal to total number of days in the fishery we are estimating catch for with a value listed as either 1 or 2 depending on whether the daytype was a weekday or weekend, respectively
  L=DayL$DayL,                                                     # decimal daylength in hrs for each day in 1 : D
  O=as.matrix(fishery.open.closed),                                # matrix denoting whether the fishery was open (1) or closed (1E-9) for each individual "D" (row) and section 1:S (column)

  #Vehicles
  V_n=nrow(V_samp),            # the total number of index effort counts  (n = index survey dates X sections X counts X angler types)
  V_I=V_samp$Count,            # sum of the index effort vehicle count within a section during an effort count
  day_V=V_samp$Day,            # an indexing column with length equal to count of Index effort counts with the day of the fishery listed starting at 1 and going to the last day n
  gear_V=V_samp$Gear,          # an indexing column with length equal to count of Index effort counts with 	a 1 or 2 depending on whether the interview was a boat or bank fisherman
  section_V=V_samp$Section,    # an indexing column with length equal to count of Index effort counts with the section number starting at 1
  countnum_V=as.numeric(V_samp$CountNum),

  #Trailers
  T_n=nrow(T_samp),            # the total number of index effort counts  (n = index survey dates X sections X # of index count for each day)
  T_I=T_samp$Count,            # sum of the index effort counts (of vehicles & trailers) within a section during an effort count
  day_T=T_samp$Day,            # an indexing column with length equal to count of Index effort counts with the day of the fishery listed starting at 1 and going to the last day n
  gear_T=T_samp$Gear,          # an indexing column with length equal to count of Index effort counts with 	a 1 or 2 depending on whether the interview was a boat or bank fisherman
  section_T=T_samp$Section,    # an indexing column with length equal to count of Index effort counts with the section number starting at 1
  countnum_T=as.numeric(T_samp$CountNum),
  
  #Angler counts
  A_n=nrow(A_samp),
  A_I=A_samp$Count,
  day_A=A_samp$Day,
  gear_A=A_samp$Gear,
  section_A=A_samp$Section,
  countnum_A=as.numeric(A_samp$CountNum),
  
  #tie ins
  E_n=nrow(E_s_samp),          # the total number of census effort counts 
  E_s=E_s_samp$Count,          # the effort count in a tie-in section
  day_E=E_s_samp$Day,          # an indexing column with length equal to count of Census effort counts with the day of the fishery listed starting at 1 and going to the last day n.
  gear_E=E_s_samp$Gear,        # an indexing column with length equal to count of Census effort counts with a 1 or 2 depending on whether the interview was a boat or bank 	fisherman
  section_E=E_s_samp$Section,  # an indexing column with length equal to count of Census effort counts 	with the section number starting at 1 
  countnum_E=as.numeric(E_s_samp$CountNum),

  #interview data - CPUE
  IntC=nrow(c_samp),              # total number of interviews conducted across all surveys dates where CPUE data (c & h) were collected 
  gear_IntC=c_samp$Gear,          # an indexing column with length equal to count of angler interviews with a 	1 or 2 depending on whether the interview was a boat or bank fisherman
  day_IntC=c_samp$Day,            # an indexing column with length equal to count angler interviews with the day of the fishery listed starting at 1 and going to the last day n.
  section_IntC = c_samp$Section.Num, #Section where angler was fishing (1 = Skagit, 2 = Sauk)
  c=c_samp[, catch.group.of.interest[1]], # total catch of [spp.of.interest] for an individual group #c_samp$SH_NOR_R          
  h=abs(c_samp$Total_Hours),        # number of hours an individual angler/group spent fishing
  
  #interview data total creeled effort and catch
  IntCreel = nrow(C_sample),
  day_Creel = C_sample$Day,
  gear_Creel = C_sample$Gear,
  section_Creel = C_sample$Section.Num,
  C_Creel = C_sample$Total_Catch,
  E_Creel = C_sample$Total_Hours,
  
  #interview data - angler expansions
  IntA=nrow(c_samp),              # total number of interviews across all surveys dates where angler expansion data (V_A, T_A, A_A) were collected 
  gear_IntA=c_samp$Gear,          # an indexing column with length equal to count of angler interviews with a 	1 or 2 depending on whether the interview was a boat or bank fisherman
  day_IntA=c_samp$Day,            # an indexing column with length equal to count angler interviews with the day of the fishery listed starting at 1 and going to the last day n.
  section_IntA = c_samp$Section.Num, #Section where angler was fishing (1 = Skagit, 2 = Sauk)
  V_A= a_samp$Vehicles,        # Total number of vehicles an individual angler group brought to the river
  T_A= a_samp$Trailers,        # Total number of Trailers an individual angler group brought to the river
  A_A= a_samp$NumAnglers       #Total number of anglers in each group interviewed
)
#standat$p_TE<-matrix(1,nrow=standat$G,ncol=standat$S)   # proportion of section covered by tie in counts: Option 1:usually 1.0, Option 2: miles of tie-in divided by total miles in section, Option 3:Best guess or something else.

```

# analysis

```{r run_analysis,results = "hide",echo=FALSE,warning=FALSE}

### Run Analysis
#Here we will run the in-season analysis of creel data to estimate CPUE, effort, and catch using the chunk of code with this heading in the RMD file.
#================================================================
#=======
#note for editing: any new priors need to go here, also in "prepare data" and in "summarize inputs"
#=======

# Denote whether you want to run a new model or load "saved" results from a previous model run
  model_source<-c("load_saved")  #enter either "run_new" or "load_saved"

# Assign a "Model_Run" number (if model_source == run_new, results will be saved to a new sub-folder; if model_source == load_saved, previous model results will be loaded)
  Model_Run<-4 #Enter numeric number (NOTE: be careful not to over-write previous models runs by entering a number that's already been used)

# Denote which creel model you want to run
  #creel_models[,1:3] #model summary table
  model_number<-c(4)
  
# Specify time period to stratify data by - day vs. week 
  model_period<-c("day") #enter "day" or "week"
  
# Specify parameter values for model priors
  value_cauchyDF_sigma_eps_C = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_eps_C; default = 1  
  value_cauchyDF_sigma_eps_E = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_eps_E; default = 1  
  value_cauchyDF_sigma_r_E = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_r_E; default = 1  
  value_cauchyDF_sigma_r_C = 0.5#1 # the hyperhyper scale (degrees of freedom) parameter in the hyperprior distribution sigma_r_C; default = 1  
  value_normal_sigma_omega_C_0 = 1  #the SD hyperparameter in the prior distribution omega_C_0; normal sd (log-space); default = 1   
  value_normal_sigma_omega_E_0 =  3 # the SD hyperparameter in the prior distribution omega_E_0; normal sd (log-space);; default = 3  
  value_lognormal_sigma_b = 1 # the SD hyperparameter in the prior distribution b; default = 1  
  value_normal_sigma_B1 = 5 # the SD hyperparameter in the prior distribution B1; default = 5  
  value_normal_mu_mu_C = log(0.02) # the mean hyperparameter in the prior distribution mu_C; median (log-space); default = 0.02 (was originally  0.05) 
  value_normal_sigma_mu_C = 1.5 # the SD hyperparameter in the prior distribution mu_C; normal sd (log-space); default = 1.5 (was originally 5)
  value_normal_mu_mu_E = log(5) # the mean hyperparameter in the prior distribution mu_E; median effort (log-space); default = 15 
  value_normal_sigma_mu_E = 2  # the SD hyperparameter in the prior distribution mu_E; normal sd (log-space); default = 2 (was originally 5) 
  value_betashape_phi_E_scaled = 1 # the rate (alpha) and shape (beta) hyperparameters in phi_E_scaled; default = 1 (i.e., beta(1,1) which is uniform), alternative beta(2,2) 
  value_betashape_phi_C_scaled = 1 # the rate (alpha) and shape (beta) hyperparameters in phi_C_scaled; default = 1 (i.e., beta(1,1) which is uniform), alternative beta(2,2)
  value_cauchyDF_sigma_mu_C = 0.5#1      # the hyperhyper SD parameter in the hyperprior distribution sigma_mu_C
  value_cauchyDF_sigma_mu_E = 0.5#1      # the hyperhyper SD parameter in the hyperprior distribution sigma_mu_E

# Specific Stan model arguments
  n_chain<-4        # set the number of Markov chains. The default is 4.
  n_iter<-6000        # set the number of iterations for each chain (including warmup). The default is 2000.
  n_cores<-4         # set the number of cores to use when executing the chains in parallel. The defaults is 1. NOTE: Stan manual recommends setting the mc.cores option to be as many processors as the hardware and RAM allow (up to the number of chains).
  n_warmup<-3000   # set the length of warm-up (aka burn-in) iterations per chain.  The default is n_iter/2.  
  n_thin<-1          # set the thinning rate (aka, the period for saving samples). The default is 1, which is usually the recommended value.
  adapt_delta<-0.999  # set adapt delta, which is the target average proposal acceptance probability during Stan's adaptation period. Value between 0 and 1. The default is 0.8. Increasing it will force stan to take smaller steps (and thus run time will be longer). 
  max_treedepth<-10 # set the max tree depth; default is 8; NOTE: this sets the max depth of tree used by NUTS during each iteration; warnings about hitting the maximum treedepth is an efficiency concern

# Create sub-folders for output (if they don't already exist)
    source(paste0(wd_source_files, "/Create_output_subfolder.R"), print.eval = TRUE)

# Run source code to prepare data for model
    source(paste0(wd_source_files, "/Prepare_Data_For_Model.R "))
# Run source code to generate creel estimates
  source(paste0(wd_source_files, "/RunNew_or_LoadSaved_Creel_Model.R"))
  
# Generate summaries of model inputs and outputs
  if(model_source == "run_new"){  source(paste0(wd_source_files, "/Summarize_Model_Inputs_and_Outputs.R"))}
```


```{r summarize_results,message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
### Summarize and Save Results
#Here we will summarize and save results of the in-season analysis of creel data to estimate CPUE, effort, and catch using the chunk of code with this heading in the RMD file.
#=============================================
#convergence diagnostics
  launch_diagnostics<-c("No") #Enter "Yes" to launch ShinyShin diagnostics
  if(launch_diagnostics=="Yes"){launch_shinystan(output$res_stan)} 

# generate plots and tables of creel estimates 
  source(file.path(wd_source_files,"Generate_Summaries_of_Creel_Estimates.R")) 
    
# KB note: update so table/plots of results are shown in PDF document
```

```{r, message=FALSE, warning=FALSE,results = "hide",echo=FALSE}
#we will quickly look at the estimated proportion of effort sampled vs estimated proportion of catch sampled
# dat<-extract(res_Stan)
# sample<-as.tibble(melt(apply(dat$E_Creel_array_gen,c(2:4),median))%>%
#   rename(section=Var1,day=Var2,gear=Var3,E=value)%>%
#   mutate(type="sample",E=as.numeric(as.character(E))))
# estimated<-as.tibble(melt(apply(dat$E,c(2:4),median)))%>%
#   rename(section=Var1,day=Var2,gear=Var3,E=value)%>%
#   mutate(type="estimate",E=as.numeric(as.character(E)))
# 
# dat<-bind_rows(sample,estimated)
# ggplot(dat,aes(x=as.numeric(day),y=E,color=type))+
#   geom_line()+
#   facet_wrap(~ as.factor(section) + as.factor(gear))
# 
# #compare psampled effort (observed) with psampled catch (estimated)
# psamp_E_obs<-melt(apply(res$E_Creel_array_gen,c(2:4),median)/apply(res$E,c(2:4),median))
# p_samp_C_est<-melt(apply(1-res$p_unsample_C,c(2:4),median))
# 
# plot(psamp_E_obs$value~p_samp_C_est$value)
# abline(a=0,b=1)
#catch sample rate is correlated with but unequal to effort sample rate!
```


## In-season Fishery Results

WDFW fishery managers use data from creel surveys to estimate the season total and daily catch and effort, and the daily catch per unit effort (CPUE). Results presented below include data collected through: **`r format(as.Date(all.Dates$Date[nrow(all.Dates)]), '%m/%d/%Y')`**

### Disclaimer
Contents on this page should be treated as preliminary and are subject to change. While every effort has been made to ensure the accuracy of data and modeling here, some numbers may change. Particularly in-season catch and effort estimates may change as additional data becomes available, and as data errors are corrected.

### Season Total Effort and Catch 

This table and the figures below it show estimates of the season total catch and effort. Estimates are shown as 'bell curves' (though not symmetric), where the height of the curve is proportional to the probability that the true value (catch or effort) is equal to a particular value on the x-axis. The "best" estimate is the 50th percentile (median) in the table, and is denoted by the middle dashed line in the graphs:

```{r run_resid_analysis, message = FALSE, warning = FALSE,results = "asis",include=TRUE, echo=FALSE}
results<-read_csv(file.path("results",catch.group.of.interest,paste0("Run_",Model_Run),"summarized_estimates",paste("Summary_Total_Catch_and_Effort",catch.group.of.interest,paste0("Run_",Model_Run),".csv",sep="_")))%>%
  dplyr::rename(Variable=X1)%>%
  dplyr::select(-CV)%>%
  #::rename(`CV (%)` = CV)%>%
  #mutate(`CV (%)`=`CV (%)`* 100)%>%
  kbl(caption = "Table 3. Total Catch (fish) and Effort (hours). The 'best' estimate is the median or 50th percentile. The table shows the percentiles of estimated catch and effort, which represent the probability that the true catch or effort is below a particular value. For example, there is a 50% probability the true catch is below the 50th percentile of the estimated catch in this table.",digits =1)%>%
  kable_classic(full_width = F, html_font = "Cambria")
print(results)
```


```{r, include=TRUE, fig.align="center", fig.cap=c("Figure 5. Season total catch (fish) and effort (angler hours). The middle dashed line shows the posterior median or 'best' estimate. Outer dashed lines show 95 percent credible intervals."),echo=FALSE,warning = FALSE}
season_results<-data.frame(res$C_sum,res$E_sum)%>%
  mutate(iter=row_number())%>%
  rename(`Season Total Catch`=res.C_sum,`Season Total Effort`=res.E_sum)%>%
  pivot_longer(names_to = "Parameter",values_to="value",cols=c(`Season Total Catch`,`Season Total Effort`))

#elimiminate very very extreme quantiles to pretty up plots
lims<-season_results%>%group_by(Parameter)%>%summarise(value = quantile(value, c(0, 0.99)), q = c(0,0.99))%>%pivot_wider(names_from = q,values_from=value)

season_results_trunc=season_results%>%left_join(lims,by="Parameter")%>%filter(value>as.numeric(`0`) & value < as.numeric(`0.99`))

ggplot(season_results_trunc,aes(x=value,fill=Parameter))+
  facet_wrap(~Parameter,ncol=1,  scales = 'free')+
  theme_bw()+
  geom_density()+
  ylab(NULL)+
  geom_vline(season_results%>%group_by(Parameter)%>%summarise(value = quantile(value, c(0.025, 0.5, 0.975)), q = c(0.025, 0.5, 0.975)),mapping=aes(xintercept=value,group=Parameter),linetype="dashed")+
  theme(legend.title = element_blank())

```

### Daily Catch, Effort, and Catch Per Unit Effort 
Below, you will find an estimate of the daily catch (fish), effort (hours), and catch per unit effort (fish/hr). The lines are the "best" estimates and the shading shows the 95 percent credible intervals (which are obtained from the statistical model used to estimate catch).
```{r, include=TRUE, fig.align="center", fig.cap=c("Figure 6. Daily catch. Lines are posterior medians or 'best' estimates, while shading shows 95% credible intervals."),echo=FALSE,warning = FALSE}
closed.Dates.DF<-closed.Dates.DF%>%pivot_longer(cols=c("Skagit","Sauk"))%>%rename(Section=name,closure=value)
Catch.summary<-Catch.summary%>%
  left_join(closed.Dates.DF,by=c("Date","Section"))%>%
  mutate(closure=ifelse(is.na(closure),"Open","Closed"))#%>%
  #mutate(Median=ifelse(closure=="Open",Median,NA),Mean=ifelse(closure=="Open",Mean,NA),l95=ifelse(closure=="Open",l95,NA),u95=ifelse(closure=="Open",u95,NA))
ggplot(Catch.summary,aes(x=as.Date(Date),y=Median,col=as.factor(Gear)))+
  facet_wrap(~Section,ncol=1)+
  geom_line(size=1.2)+
  ylab(paste0("Catch - ",catch.group.of.interest," (fish)"))+
  xlab("")+
  scale_x_date(date_labels = "%b-%d",date_breaks = "weeks")+
  geom_ribbon(aes(ymin=l95, ymax=u95,fill=as.factor(Gear)),alpha=0.2,col=NA)+
  #geom_rect(Catch.summary%>%filter(closure=="Closed"),mapping=aes(xmin = as.Date(Date)-0.5, xmax = as.Date(Date)+0.5, fill = as.factor(closure)),color=NA, ymin = -Inf, ymax = Inf, alpha = 1,fill="grey90")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90),legend.title = element_blank())
```

```{r, include=TRUE, fig.align="center", fig.cap=c("Figure 7. Daily effort. Lines are posterior medians or 'best' estimates, while shading shows 95% credible intervals."), echo=FALSE,warning=F}
Effort.summary<-Effort.summary%>%
  left_join(closed.Dates.DF,by=c("Date","Section"))%>%
  mutate(closure=ifelse(is.na(closure),"Open","Closed"))#%>%
  #mutate(Median=ifelse(closure=="Open",Median,NA),Mean=ifelse(closure=="Open",Mean,NA),l95=ifelse(closure=="Open",l95,NA),u95=ifelse(closure=="Open",u95,NA))
ggplot(Effort.summary,aes(x=as.Date(Date),y=Median,col=as.factor(Gear)))+
  facet_wrap(~Section,ncol=1)+
  geom_line(size=1.2)+
  ylab("Angling Effort (hrs)")+
  xlab("")+
  scale_x_date(date_labels = "%b-%d",date_breaks = "weeks")+
  geom_ribbon(aes(ymin=l95, ymax=u95,fill=as.factor(Gear)),alpha=0.2,col=NA)+
  #geom_rect(Effort.summary%>%filter(closure=="Closed"),mapping=aes(xmin = as.Date(Date)-0.5, xmax = as.Date(Date)+0.5, fill = as.factor(closure)),color=NA, ymin = -Inf, ymax = Inf, alpha = 0.2,fill="grey80")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90),legend.title = element_blank())
```

```{r, include=TRUE, fig.align="center", fig.cap=c("Figure 8. Daily CPUE. Lines are posterior medians or 'best' estimates, while shading shows 95% credible intervals."),echo=FALSE,warning=F}
CPUE.summary<-CPUE.summary%>%
  left_join(closed.Dates.DF,by=c("Date","Section"))%>%
  mutate(closure=ifelse(is.na(closure),"Open","Closed"))#%>%
  #mutate(Median=ifelse(closure=="Open",Median,NA),Mean=ifelse(closure=="Open",Mean,NA),l95=ifelse(closure=="Open",l95,NA),u95=ifelse(closure=="Open",u95,NA))
ggplot(CPUE.summary,aes(x=as.Date(Date),y=Median,col=as.factor(Gear)))+
  facet_wrap(~Section,ncol=1)+
  geom_line(size=1.2)+
  ylab("Catch Per Unit Effort (fish/hrs)")+
  xlab("")+
  scale_x_date(date_labels = "%b-%d",date_breaks = "weeks")+
  geom_ribbon(aes(ymin=l95, ymax=u95,fill=as.factor(Gear)),alpha=0.2,col=NA)+
  #geom_rect(CPUE.summary%>%filter(closure=="Closed"),mapping=aes(xmin = as.Date(Date)-0.5, xmax = as.Date(Date)+0.5, fill = as.factor(closure)),color=NA, ymin = -Inf, ymax = Inf, alpha = 0.2,fill="grey80")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90),legend.title = element_blank())
```

### In-season Impact Monitoring: Fishery Impacts Relative to Resource Management Plan Limits
Here, we compare the actual in-season estimated sport fishery catch, assuming 10% C&R mortality, with the allowable harvest estimated from the forecasted run size and the RMP mortality limits, to estimate the probability that the allowable harvest has been exceeded by the actual fishery (as opposed to the pre-season plan that we looked at previously). Early in the season when little catch has occurred, the probability that the allowable harvest has been exceeded is low, but increases as the season goes on. This analyis enables an in-season quantification of risk by managers, which may be helpful in decision-making:
```{r include=TRUE, fig.align="center", fig.cap=c("Figure 9. Probability that the actual in-season estimated sport fishery harvest exceeds the allowable harvest (expressed as percentages) under the RMP. Vertical lines denote 50 percent of the allowable harvest, which is shared between the state and tribes, and 100 percent of the allowable harvest. The best in-season estimate of the percent of the allowable harvest that has been used by the sport fishery is located on the graph where the probability of exceedance is 50 percent."),echo=FALSE,warning=F}
H<-res$C_sum*0.1
PAH<-(sample(H,10000,replace = T)/sample(AH$AH,10000,replace = T))*100
PAHdat<-data.frame(PAH,percent_rank(-PAH))%>%rename(Exceedence="percent_rank..PAH.")
ggplot(data=PAHdat,aes(x=PAH,y=Exceedence))+
  geom_line(size=1.25)+
  theme_bw()+
  ylab("Probability of Exceedance")+
  xlab("% of Allowable (Sport + Tribal) Co-Manager Harvest")+
  xlim(0,max(quantile(PAHdat$PAH,0.99),100))+
  geom_vline(xintercept=50)+
  geom_vline(xintercept=100,col="red")
```

```{r include=TRUE, fig.align="center", fig.cap=c("Figure 10. Probability that the sport fishery harvest exceeds 50 and 100 percent of the allowable harvest under the RMP, as well as the best estimate of the percentage of allowable impacts the sport fishery will use."),echo=FALSE}
probs<-c(PAHdat$PAH[which(abs(PAHdat$Exceedence - 0.5)==min(abs(PAHdat$Exceedence - 0.5)))],
  PAHdat$Exceedence[which(abs(PAHdat$PAH - 50)==min(abs(PAHdat$PAH - 50)))]*100,
  PAHdat$Exceedence[which(abs(PAHdat$PAH - 100)==min(abs(PAHdat$PAH - 100)))]*100
  )
par(mar=c(5,15,5,5))
names(probs)<-c("best estimate of sport \nfishery impacts as \n % of total (sport + tribal) \n allowable","probability sport fishery \n impacts > 50% of total \n (sport + tribal) allowable","probability sport fishery \n impacts > 100% of total \n (sport + tribal) allowable")
barplot(probs,horiz = T,xlim=c(0,100),las=2,yaxs="i",col="forest green", main="In-season Estimate"
        )
box()
```

***

Page Last Updated: `r format(Sys.time(), '%m/%d/%Y')`.

***